# ep18g6

Files

-- scanner.l --  
Original file.

-- scanner.c --  
Scanner generated by flex with default parameters (slightly cleaned up). 
Uses some techniques to compress tables, not quite sure how it works.  
~ 4.2B cycles (with -O3) on g0.complang.tuwien.ac.at

-- scanner-fast.c --  
Output of 'flex --fast scanner.l'  
~ 2.3B cycles

-- scanner-full.c --  
Output of 'flex --full scanner.l'
Fast and full seem about equally fast, don't quite understand how --fast functions. 
Full is simple though.  
~ 2.3B cycles

-- scanner2.l --  
Changed file so we don't call lex() repeatedly but only once and return once hash 
is fully computed.  
~ 2.05B cycles, ~2.5B instructions compared to ~3B for the two preceding.

-- scanner3.l --  
Read whole file into buffer before handing buffer to flex-generated scanner. 
Is actually slower than letting the scanner handle it, at least with fread().  
~ 2.2B cycles


-- lexer.c --  
First try to implement a simple DFA (something like in dfa1.png) using a loop 
and nested switch statements. 
Keywords are not recognized by DFA as that really inflates the number of states.
Instead treat them together with identifiers and test for each in a loop. 
No real regard for speed here, just wanted something that works correctly. Turned
out slower than the (full&fast) generated scanners.

-- lexer2.c --  
Forget about simulating a DFA, just distinguish 8 cases for each first char and then 
handle possible sub-cases. Keywords are still checked for in a loop here.  
~ 1.6B cycles, 1.8B instructions

-- lexer2b.c --  
Added improvement to determining whether we have keyword or identifier. 
Hash is computed as if we have ident, bits 2-7 of hash will be different for all 
keywords. So use lookup table to check whether we indeed have a keyword and get
its value.  
~ 1.5B cycles, 1.6B instructions

-- lexer2bm.c --  
Same as above but use mmap for loading file, noticeably improves performance in
this case.  
~ 1.35B cycles, 1.55B instructions

-- lexer22m.c --  
Tried dispatch with jump tables, also added zero guard to end of buffer
Rather large number of branch mis-predictions slow this one down.  
~ 1.4B cycles, 1.15B instructions

-- lexer23m.c --  
Builds in 2b, added guard at end of buffer to eliminate < comparison. 
Pulled check for lexemchar of switch. Not sure of individual effects.  
~ 1.3B cycles, 1.35B instructions

-- lexer24m.c --  
Slight changes to loop to read ahead a character, check for letter also put outside 
of switch statement as very common case. Check for assign also slightly optimized. 
Use table lookup instead of isalnum() to find end of identifier/keyword. 
Fixing some variables to 64 bit also eliminated some widening instructions gcc 
created otherwise.  
Too many changes at once to determine impact of each, needs more testing...  
~ 0.99B cycles, 1.25B instructions, also > 9% branch misses

-- lexer27m.c --  
Split type lookup into classes for current character and look ahead character. 
But also some reordering of comparisons -- impact of each? 
Simplified end of comment check by adding new line at end, will be small impact.  
~ 0.97B cycles, 1.27B instructions, ~9% branch misses

-- lexer27d.c --  
Same as above but with a jump table, quite a bit slower again.  
~ 1.3B cycles, 1.15B instructions, 16% branch misses

-- lexer27am.c --  
Alternate way to determine action up front with large table and next two input 
characters. Slower.  
~ 1.15B cycles

-- lexer27mx.c --  
Replace one branch in keyword check with conditional move instruction. This reduces
the number of branches by around 20M though the number of absolute misses stays the
same.  
~ 0.94B cycles, 1.22B instructions, ~9.5% branch misses

-- lexer2qd.c & lexer2dasm.s --  
Attempting jump table dispatch again with simplest mechanism, just look up location
in table using current character and jump to handler. Could be viable since total
number of instructions is very low, have not tried mmap version yet.  
Very low number of branches too, though relative misses very high. Basically branch
prediction does not work at all for the table jumps so can expect 50% misses for 
each processed token.  
~ 1.15B cycles, 0.85B instructions, 137M branches, >22% branch misses  
Assembler version is just gcc output cleaned up a bit.  
~ 1.12B cycles, 0.72B instructions, 120M branches, ~25% branch misses

-- lexer3.c --  
Implement roughly the same DFA as lexer.c using tables similar to the scanners 
generated by flex using --full, but also using equivalence classes to reduce size. 
All -3X scanners currently only tested without mmap...  
~ 1.75B cycles, 2.45B instructions


-- lexer31.c --  
Full scanner recognizing keywords too, significantly larger table with ~50 states 
and 29 equivalence classes. Slow because of low instruction throughput.  
~ 1.9B cycles, 2.1B instructions, ~10% branch misses

-- lexer32.c --  
Broadly identical? Just some renames and tried to change loop?  
~ 1.96B cycles, 2.05B instructions, >8% branch misses

-- lexer34.c --  
Even bigger table with 63 states and 38 classes to recognize different states for 
different lexemchars. Different way to return and use table values. Table states 
also made 16 bits, so getting pretty huge.  
Keywords and lexem-chars can be treated together, also transitions from whitespace 
to lexem don't need to break out of loop.  
Still need to break out in other cases to get start of token for parsing numbers 
and identifiers...  
All of these lexers have the problem that after each recognized token we have 
to return to the start state and reparse the last character.
Also I note that while I managed to improve the number of instructions, the number
of cycles actually has not improved at all.  
~ 1.78B cycles, 1.77B instructions ~9% branch misses
Correction:  
These measurements seem to be wrong somehow, now I get:  
~ 1.55B cylces, 1.83B instructions  

TODO: full (7-bit) table; return state offset directly since already 16-bit


## -- Possible Improvements, Questions --

What is the most efficient way to load the file in the first place? Mmap seems 
good, but maybe it's actually faster to process it in smaller chunks?

Some clever way to make a table based DFA faster?
Or reduce table size if cache misses are huge factor...?

Possible to process more than one character at a time? Can vectorization be applied anywhere?

Maybe two passes over input could be faster than one that does everything?
Tokenize first somehow then further process tokens?

Probably good idea to measure exactly where time is actually spent...

...

ADD MORE
